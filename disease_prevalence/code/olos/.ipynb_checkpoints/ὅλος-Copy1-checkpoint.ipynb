{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "from pytrends.request import TrendReq\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import matplotlib.pyplot as plt\n",
    "from queue import Queue\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wikipedia\n",
    "import pytrends\n",
    "import requests\n",
    "import vincent\n",
    "import random\n",
    "import spacy\n",
    "import json\n",
    "import time\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import linear_model as lm\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# in this last notebook we developed an automatic procedure to analyze and build regression models for prevalence data using google trends and census features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TOPICS= {\"arthritis\": [\"05\", [[\"HAVARTH\", 2001], [\"HAVARTH2\", 2003], [\"_DRDXART\", 2005], [\"_DRDXART\", 2007],\n",
    "                                [\"_DRDXART\", 2009], [\"_DRDXAR1\", 2011], [\"_DRDXAR1\", 2012], [\"_DRDXAR1\", 2013], \n",
    "                                [\"_DRDXAR1\", 2014], [\"_DRDXAR1\",2015], [\"_DRDXAR1\", 2016]]], \n",
    "           \"depression\": [\"17\", [[\"ADDEPEV2\", 2011], [\"ADDEPEV2\", 2012], [\"ADDEPEV2\", 2013], \n",
    "                                 [\"ADDEPEV2\", 2014], [\"ADDEPEV2\", 2015], [\"ADDEPEV2\", 2016]]],\n",
    "           \"asthma\": [\"06\", [[\"_CASTHMA\", 2000], [\"_CASTHMA\", 2001], [\"_CASTHMA\", 2002], [\"_CASTHMA\", 2003], \n",
    "                      [\"_CASTHMA\", 2004], [\"_CASTHMA\", 2005], [\"_CASTHMA\", 2006], [\"_CASTHMA\", 2007], \n",
    "                      [\"_CASTHMA\", 2008], [\"_CASTHMA\", 2009], [\"_CASTHMA\", 2010], [\"_CASTHM1\", 2011], \n",
    "                      [\"_CASTHM1\", 2012], [\"_CASTHM1\", 2013], [\"_CASTHM1\", 2014], [\"_CASTHM1\", 2015], [\"_CASTHM1\", 2016]]\n",
    "                     ],\n",
    "           \"cardiovascular\": [\"10\" , [[\"CVDCRHD3\", 2005], [\"CVDCRHD3\", 2006], [\"CVDCRHD4\", 2007], [\"CVDCRHD4\", 2008], [\"CVDCRHD4\", 2009], \n",
    "                              [\"CVDCRHD4\", 2010], [\"CVDCRHD4\", 2011], [\"CVDCRHD4\", 2012], [\"CVDCRHD4\", 2013], \n",
    "                             [\"CVDCRHD4\", 2014], [\"CVDCRHD4\", 2015], [\"CVDCRHD4\", 2016]]],\n",
    "           \"COPD\": [\"14\", [[\"CHCCOPD\", 2011], [\"CHCCOPD1\", 2012], [\"CHCCOPD1\", 2013], [\"CHCCOPD1\", 2014], [\"CHCCOPD1\", 2015],\n",
    "                          [\"CHCCOPD1\", 2016]]],\n",
    "           \"diabetes\": [\"18\", [[\"DIABETES\", 1995], [\"DIABETES\", 1996], [\"DIABETES\", 1997], [\"DIABETES\", 1998], [\"DIABETES\", 1999],\n",
    "                               [\"DIABETES\", 2000], [\"DIABETES\", 2001], [\"DIABETES\", 2002], [\"DIABETES\", 2003], \n",
    "                               [\"DIABETE2\", 2004], [\"DIABETE2\", 2005], [\"DIABETE2\", 2006], [\"DIABETE2\", 2007],\n",
    "                               [\"DIABETE2\", 2008], [\"DIABETE2\", 2009], [\"DIABETE2\", 2010], [\"DIABETE3\", 2011],\n",
    "                               [\"DIABETE3\", 2012],  [\"DIABETE3\", 2013],  [\"DIABETE3\", 2014],  [\"DIABETE3\", 2015],\n",
    "                               [\"DIABETE3\", 2016]]],\n",
    "           \"kidney\": [\"35\", [[\"CHCKIDNY\", 2011], [\"CHCKIDNY\", 2012], [\"CHCKIDNY\", 2013], [\"CHCKIDNY\", 2014], \n",
    "                             [\"CHCKIDNY\", 2015], [\"CHCKIDNY\", 2016]]],\n",
    "\n",
    "           \"other_cancer\": [\"40\", [[\"CHCOCNCR\", 2011], [\"CHCOCNCR\", 2012], [\"CHCOCNCR\", 2013], [\"CHCOCNCR\", 2014], \n",
    "                            [\"CHCOCNCR\", 2015], [\"CHCOCNCR\", 2016]]],\n",
    "           \"skin_cancer\": [\"53\", [[\"CHCSCNCR\", 2011], [\"CHCSCNCR\", 2012], [\"CHCSCNCR\", 2013], [\"CHCSCNCR\", 2014],\n",
    "                          [\"CHCSCNCR\", 2015], [\"CHCSCNCR\", 2016]]],\n",
    "           #\"vision\": [\"62\", [[\"CHCVISN1\", 2011], [\"CHCVISN1\", 2012]]]\n",
    "          }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# this class retrieve the prevalence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Prevalence:\n",
    "    '''\n",
    "    class to extract prevalence\n",
    "    '''\n",
    "    def __init__(self, diseases):\n",
    "        self.data_temp=pd.DataFrame()\n",
    "        self.data_tot=pd.DataFrame()\n",
    "        self.ground_truth = [\"AgeAdjusted\", \"Crude\"]\n",
    "        self.topics=diseases\n",
    "        \n",
    "    def retrieve_prevalence(self, disease=\"depression\", p=\"c\"):\n",
    "        self.data_temp=pd.DataFrame()\n",
    "        self.data_tot=pd.DataFrame()\n",
    "        \n",
    "        topic=self.topics[disease][0]\n",
    "        years=self.topics[disease][1]\n",
    "        if p == \"c\":\n",
    "            value=self.ground_truth[1]\n",
    "        else:\n",
    "            value=self.ground_truth[0]\n",
    "        \n",
    "        for y in range(len(years)):\n",
    "            \n",
    "            code=years[y][0]\n",
    "            year=years[y][1]\n",
    "            print(year)\n",
    "            query=\"https://nccd.cdc.gov/BRFSSPrevalence/rdPage.aspx?rdReport=DPH_BRFSS.ExportData&DataType=StatesAndMMSA&ClassCode=CLASS03&TopicCode=TOPIC\"\\\n",
    "            + str(topic) + \"&StratTypeCode=CAT1&StratCode=&LocationCode=&IndicatorCode=\" \\\n",
    "            + code +\"&ResponseCode=RESP046&QueryType=Chart&YearStart=\" \\\n",
    "            + str(year) + \"&YearEnd=&DataValueType=\"\\\n",
    "            + str(value) + \"&ShowMMSA=false&rdReportFormat=CSV&rdExportTableID=dtExport&rdExportFilename=ExportCSV\"\n",
    "            \n",
    "            self.routine(query)\n",
    "            self.data_tot=pd.concat([self.data_tot, self.data_temp], axis=0)\n",
    "        return self.data_tot\n",
    "        \n",
    "    def routine(self, query):\n",
    "        r=requests.get(query)\n",
    "        data=r.text.split(\"\\r\\n\")\n",
    "        #print(data)\n",
    "        for r in range(len(data)):\n",
    "            x=data[r].split(\",\")\n",
    "            data[r] = [i.strip(\"\\\"\") for i in x[:28]]\n",
    "        data_df=pd.DataFrame(data[3:54], columns=data[0])\n",
    "        self.data_temp=data_df[[\"LocationDesc\", \"Data_Value\", \"Year\"]]\n",
    "        \n",
    "    def run(self, disease=\"depression\"):\n",
    "        print(\"crude\")\n",
    "        p_c=self.retrieve_prevalence(disease, \"c\")\n",
    "        try:\n",
    "            print(\"adjusted\")\n",
    "            p_a=self.retrieve_prevalence(disease, \"a\")\n",
    "        except:\n",
    "            p_a=None\n",
    "        return p_c, p_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prevalence=Prevalence(TOPICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diseases=[\"arthritis\", \"depression\", \"asthma\", \"cardiovascular\", \"COPD\", \"diabetes\", \"kidney\", \"other_cancer\", \"skin_cancer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for d in diseases:\n",
    "    print(d)\n",
    "    p_c, p_a=prevalence.run(d)\n",
    "    p_c.to_csv(\"./data/ground_truth/\"+d+\"_c.csv\")\n",
    "    if p_a is not None:\n",
    "        p_a.to_csv(\"./data/ground_truth/\"+d+\"_a.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#d=wikipedia.page(\"Depression disease\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate features with Google Trends & census valiables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Trends:\n",
    "    '''\n",
    "    build the features matrix using google trends data and add census data\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, parser, index, years, queries, n=30, features=[], explored=set()):\n",
    "        \n",
    "        self.n=n                  # number of features\n",
    "        self.index = index\n",
    "        self.years = years\n",
    "        self.parser = parser\n",
    "        self.queries=queries      # initial query list\n",
    "        self.features = features  # list of features\n",
    "        self.explored = explored  # explored nodes\n",
    "        self.matrix_features={}\n",
    "        self.matrix_df=pd.DataFrame()\n",
    "        self.position_df=pd.DataFrame()\n",
    "        self.unemployment_df=pd.DataFrame()\n",
    "        self.insurance_df=pd.DataFrame()\n",
    "        self.income_df=pd.DataFrame()\n",
    "        \n",
    "    def getFeatures(self):\n",
    "        return self.features\n",
    "    def getExplored(self):\n",
    "        return self.explored\n",
    "    def getMatrix_df(self):\n",
    "        return self.matrix_df\n",
    "    \n",
    "    def routine_features(self, text):\n",
    "        \n",
    "        text = [i[0]+\" \" for i in text]\n",
    "        text = ''.join(text)\n",
    "        for token in self.parser(text):\n",
    "            #print(token.text, token.pos_)\n",
    "            if token.pos_ == \"NOUN\":\n",
    "                #print(token.text, token.pos_)\n",
    "                self.features.append(token.text)\n",
    "        self.features=list(set(self.features))\n",
    "        \n",
    "    def build_features(self):\n",
    "        \n",
    "        '''generate a list of n nouns related with the initial queries'''\n",
    "\n",
    "        queue = Queue()\n",
    "        #explored=set()\n",
    "        for q in self.queries:\n",
    "            if q not in self.explored:\n",
    "                queue.put(q)\n",
    "                \n",
    "        print(\"build features\")\n",
    "        while (len(self.features) < self.n) and (not queue.empty()):\n",
    "            q=queue.get()\n",
    "            self.explored.add(q)\n",
    "            print(q)\n",
    "            pytrends = TrendReq(hl='en-US', tz=360)\n",
    "            ## cat  46 health\n",
    "            pytrends.build_payload([q], cat=45, timeframe='today 5-y', geo='US', gprop='')\n",
    "            related_queries=pytrends.related_queries()\n",
    "            keys=related_queries[q].keys()\n",
    "\n",
    "            for k in keys:\n",
    "                text = related_queries[q][k].values\n",
    "                self.routine_features(text)\n",
    "                temp=set(self.features)\n",
    "                temp=temp.difference(self.explored)\n",
    "                temp=list(temp)\n",
    "                for f in temp:\n",
    "                    ## just to be safer\n",
    "                    if f not in self.explored:\n",
    "                        queue.put(f)\n",
    "            #time.sleep(10)\n",
    "\n",
    "        self.features = list(set(self.features + self.queries))\n",
    "        \n",
    "    def build_matrix(self):\n",
    "        \n",
    "        '''compute the google trend for every feature and every year'''\n",
    "        \n",
    "        self.matrix_features = {f:[] for f in self.features}\n",
    "        self.matrix_features[\"year\"] = []\n",
    "        pytrends = TrendReq(hl='en-US', tz=360)\n",
    "        print(\"build matrix\")\n",
    "        for year in self.years:\n",
    "            if year > 2003:\n",
    "                print(year)\n",
    "                for f in self.features:\n",
    "                    try:\n",
    "                        pytrends.build_payload([f], cat=0, timeframe=str(year) + \"-01-01 \" + str(year+1) + \"-12-31\", geo='US', gprop='')\n",
    "                        x = pytrends.interest_by_region(resolution='COUNTRY').values\n",
    "                        x = [i[0] for i in x]\n",
    "                        if len(x) == len(self.index):\n",
    "                            self.matrix_features[f].extend(x[:])\n",
    "                        else:\n",
    "                            self.matrix_features[f].extend([None for _ in range(len(self.index))])\n",
    "                    except:\n",
    "                        self.matrix_features[f].extend([None for _ in range(len(self.index))])\n",
    "                self.matrix_features[\"year\"].extend([year for _ in range(len(self.index))])\n",
    "                #time.sleep(1)\n",
    "\n",
    "        index_all=[]\n",
    "        for y in self.years:\n",
    "            if y > 2003:\n",
    "                index_all.extend(self.index)\n",
    "\n",
    "        self.matrix_df = pd.DataFrame(self.matrix_features, index=index_all)\n",
    "        \n",
    "        \n",
    "    def process_data(self):\n",
    "        '''\n",
    "        process income, unemployment, insurance and position data\n",
    "        '''\n",
    "\n",
    "        position=\"./data/positions.csv\"\n",
    "        income=\"./data/income/income_clean.csv\"\n",
    "        insurance=\"./data/insurance/insurance_clean.csv\"\n",
    "        unemployment=\"./data/unemployment/unemployment.csv\"\n",
    "        self.position_df=pd.read_csv(position)\n",
    "        self.unemployment_df=pd.read_csv(unemployment)\n",
    "        self.insurance_df=pd.read_csv(insurance, index_col=0)\n",
    "        self.income_df=pd.read_csv(income, index_col=0, header=None)\n",
    "        \n",
    "        for y in self.years:\n",
    "            if y > 2003:\n",
    "                if y < 2013:\n",
    "                    self.insurance_df[str(y)] = None\n",
    "                if y < 2011:\n",
    "                    self.position_df[str(y)] = None\n",
    "                    d={\"State\": [i for i in self.index], \"Rate\": [None for _ in self.index], \"year\": [y for _ in self.index]}\n",
    "                    d=pd.DataFrame(d, columns=[\"State\", \"Rate\", \"year\"])\n",
    "                    self.unemployment_df=pd.concat([d, self.unemployment_df], axis=0)\n",
    "                \n",
    "        self.insurance_df=self.insurance_df[[str(y) for y in self.years if y > 2003]]\n",
    "\n",
    "        years_all = [i for i in range(2016, 1999, -1)]\n",
    "        cols=[]\n",
    "        for i in years_all:\n",
    "            cols.append(\"median_\"+str(i))\n",
    "            cols.append(\"st_\"+str(i))\n",
    "\n",
    "        self.income_df=pd.DataFrame(self.income_df.values, index=self.income_df.index, columns=cols)\n",
    "\n",
    "    def update_matrix(self):\n",
    "        '''\n",
    "        update with census data, index states and insurance rate\n",
    "        '''\n",
    "        \n",
    "        self.process_data()\n",
    "        insurance=pd.Series()\n",
    "        income=pd.Series()\n",
    "        index_states=[i for i in range(len(self.index))]\n",
    "        index_states_all=[]\n",
    "        print(\"update matrix\")\n",
    "        for y in self.years:\n",
    "            if y > 2003:\n",
    "                try:\n",
    "                    insurance = pd.concat([insurance, self.insurance_df[str(y)]], axis=0)\n",
    "                except:\n",
    "                    continue\n",
    "                try:\n",
    "                    income = pd.concat([income, self.income_df[\"median_\" + str(y)]], axis=0)\n",
    "                except:\n",
    "                    continue\n",
    "                index_states_all = index_states_all + index_states\n",
    "\n",
    "        # update matrix\n",
    "        self.matrix_df[\"states\"] = index_states_all\n",
    "        self.matrix_df[\"income\"] = income.values\n",
    "        self.matrix_df[\"insurance\"] = insurance.values\n",
    "        self.matrix_df[\"unemployment\"] = self.unemployment_df[\"Rate\"].values\n",
    "        #self.matrix_df[\"latitude\"] = self.position_df[\"Latitude\"].values\n",
    "        #self.matrix_df[\"longitude\"] = self.position_df[\"Longitude\"].values\n",
    "\n",
    "        # remove features with more that 200 nan\n",
    "        remove=list(self.matrix_df.columns[self.matrix_df.isnull().sum()>200])\n",
    "        self.matrix_df.drop(remove, axis=1, inplace=True)\n",
    "        \n",
    "    def run(self):\n",
    "        self.build_features()\n",
    "        self.build_matrix()\n",
    "        self.update_matrix()\n",
    "        print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_c=pd.read_csv(\"./data/ground_truth/depression_c.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index=sorted(set(p_c[\"LocationDesc\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def routine(p, years, index):\n",
    "    y=[]\n",
    "    for year in years:\n",
    "        if year > 2003:\n",
    "            x=p[p[\"Year\"]==year][\"Data_Value\"].values\n",
    "            if len(x) != len(index):\n",
    "                y.extend([None for _ in range(len(index))])\n",
    "            else:\n",
    "                y.extend(x)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we cannot run all together because google trends doesn't allow scraping (obviously)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arthritis',\n",
       " 'depression',\n",
       " 'asthma',\n",
       " 'cardiovascular',\n",
       " 'COPD',\n",
       " 'diabetes',\n",
       " 'kidney',\n",
       " 'other_cancer',\n",
       " 'skin_cancer']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diseases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "asthma=diseases[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'asthma'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asthma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "years= [y[1] for y in TOPICS[asthma][1] if y[1] > 2003]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trends=Trends(parser, index, years, [asthma], n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trends.build_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trends.getFeatures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trends.build_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trends.update_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matrix_asthma=pd.read_csv(\"./data/matrices_features/matrix_asthma.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(663, 35)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_asthma.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>albuterol</th>\n",
       "      <th>allergy</th>\n",
       "      <th>associates</th>\n",
       "      <th>asthma</th>\n",
       "      <th>attack</th>\n",
       "      <th>breathing</th>\n",
       "      <th>center</th>\n",
       "      <th>code</th>\n",
       "      <th>copd</th>\n",
       "      <th>cough</th>\n",
       "      <th>...</th>\n",
       "      <th>sinus</th>\n",
       "      <th>symptoms</th>\n",
       "      <th>test</th>\n",
       "      <th>thunderstorm</th>\n",
       "      <th>treatment</th>\n",
       "      <th>what</th>\n",
       "      <th>year</th>\n",
       "      <th>states</th>\n",
       "      <th>income</th>\n",
       "      <th>prevalence_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Alabama</th>\n",
       "      <td>43</td>\n",
       "      <td>80</td>\n",
       "      <td>54</td>\n",
       "      <td>83</td>\n",
       "      <td>95</td>\n",
       "      <td>63</td>\n",
       "      <td>64</td>\n",
       "      <td>78</td>\n",
       "      <td>90</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>80</td>\n",
       "      <td>88</td>\n",
       "      <td>90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85</td>\n",
       "      <td>81</td>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>36629</td>\n",
       "      <td>8.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alaska</th>\n",
       "      <td>34</td>\n",
       "      <td>67</td>\n",
       "      <td>35</td>\n",
       "      <td>83</td>\n",
       "      <td>82</td>\n",
       "      <td>63</td>\n",
       "      <td>62</td>\n",
       "      <td>69</td>\n",
       "      <td>40</td>\n",
       "      <td>50</td>\n",
       "      <td>...</td>\n",
       "      <td>70</td>\n",
       "      <td>71</td>\n",
       "      <td>84</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84</td>\n",
       "      <td>72</td>\n",
       "      <td>2004</td>\n",
       "      <td>1</td>\n",
       "      <td>55063</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arizona</th>\n",
       "      <td>40</td>\n",
       "      <td>77</td>\n",
       "      <td>56</td>\n",
       "      <td>59</td>\n",
       "      <td>77</td>\n",
       "      <td>57</td>\n",
       "      <td>54</td>\n",
       "      <td>88</td>\n",
       "      <td>37</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>61</td>\n",
       "      <td>76</td>\n",
       "      <td>79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77</td>\n",
       "      <td>73</td>\n",
       "      <td>2004</td>\n",
       "      <td>2</td>\n",
       "      <td>43846</td>\n",
       "      <td>7.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arkansas</th>\n",
       "      <td>41</td>\n",
       "      <td>87</td>\n",
       "      <td>51</td>\n",
       "      <td>59</td>\n",
       "      <td>78</td>\n",
       "      <td>84</td>\n",
       "      <td>54</td>\n",
       "      <td>77</td>\n",
       "      <td>37</td>\n",
       "      <td>46</td>\n",
       "      <td>...</td>\n",
       "      <td>84</td>\n",
       "      <td>86</td>\n",
       "      <td>98</td>\n",
       "      <td>NaN</td>\n",
       "      <td>81</td>\n",
       "      <td>86</td>\n",
       "      <td>2004</td>\n",
       "      <td>3</td>\n",
       "      <td>34984</td>\n",
       "      <td>7.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>California</th>\n",
       "      <td>28</td>\n",
       "      <td>68</td>\n",
       "      <td>57</td>\n",
       "      <td>59</td>\n",
       "      <td>86</td>\n",
       "      <td>54</td>\n",
       "      <td>63</td>\n",
       "      <td>97</td>\n",
       "      <td>31</td>\n",
       "      <td>35</td>\n",
       "      <td>...</td>\n",
       "      <td>52</td>\n",
       "      <td>66</td>\n",
       "      <td>77</td>\n",
       "      <td>NaN</td>\n",
       "      <td>74</td>\n",
       "      <td>71</td>\n",
       "      <td>2004</td>\n",
       "      <td>4</td>\n",
       "      <td>49222</td>\n",
       "      <td>7.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            albuterol  allergy  associates  asthma  attack  breathing  center  \\\n",
       "Alabama            43       80          54      83      95         63      64   \n",
       "Alaska             34       67          35      83      82         63      62   \n",
       "Arizona            40       77          56      59      77         57      54   \n",
       "Arkansas           41       87          51      59      78         84      54   \n",
       "California         28       68          57      59      86         54      63   \n",
       "\n",
       "            code  copd  cough      ...       sinus  symptoms  test  \\\n",
       "Alabama       78    90     43      ...          80        88    90   \n",
       "Alaska        69    40     50      ...          70        71    84   \n",
       "Arizona       88    37     43      ...          61        76    79   \n",
       "Arkansas      77    37     46      ...          84        86    98   \n",
       "California    97    31     35      ...          52        66    77   \n",
       "\n",
       "            thunderstorm  treatment  what  year  states  income  prevalence_c  \n",
       "Alabama              NaN         85    81  2004       0   36629           8.6  \n",
       "Alaska               NaN         84    72  2004       1   55063           9.0  \n",
       "Arizona              NaN         77    73  2004       2   43846           7.2  \n",
       "Arkansas             NaN         81    86  2004       3   34984           7.4  \n",
       "California           NaN         74    71  2004       4   49222           7.7  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_asthma.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for d in diseases:\n",
    "    try:\n",
    "        print(d)\n",
    "        years= [y[1] for y in TOPICS[d][1] if y[1] > 2003]\n",
    "        queries=[d]\n",
    "        trends=Trends(parser, index, years, queries, n=30)\n",
    "        trends.run()\n",
    "        matrix_df=trends.getMatrix_df()\n",
    "        \n",
    "        p_c=pd.read_csv(\"./data/ground_truth/\"+d+\"_c.csv\", index_col=0)\n",
    "        matrix_df[\"prevalence_c\"] = routine(p_c, years, index)\n",
    "        try:\n",
    "            p_a=pd.read_csv(\"./data/ground_truth/\"+d+\"_c.csv\", index_col=0)\n",
    "            matrix_df[\"prevalence_a\"] = routine(p_a, years, index)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        matrix_df.to_csv(\"./data/matrices_features/matrix_\"+d+\".csv\")\n",
    "        time.sleep(10)\n",
    "    except:\n",
    "        print(\"problem with \"+ d)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    '''\n",
    "    model selection\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, matrix_df, l=[], flag=\"c\", r=10, cv_type=\"k\", test_size=0.2, k=3):\n",
    "        \n",
    "        self.matrix_df=matrix_df\n",
    "        self.test_size=test_size\n",
    "        # cv type: k-fold or loo\n",
    "        self.cv_type=cv_type\n",
    "        # crude or age-adj prevalence\n",
    "        self.flag=flag\n",
    "        ## list features to remove\n",
    "        self.l=l\n",
    "        # random state for train/test splitting\n",
    "        self.r=r\n",
    "        # number cv in grid search\n",
    "        self.k=k\n",
    "        # data structures\n",
    "        self.X=[]\n",
    "        self.y_c=[]\n",
    "        self.y_a=[]\n",
    "        self.X_train=[]\n",
    "        self.y_train=[]\n",
    "        self.X_test=[]\n",
    "        self.y_test=[]\n",
    "        ### parameters for grid search\n",
    "        self.lasso={\"alpha\": [0.00001, 0.0001, 0.001, 0.01, 0.1, 1.0]}\n",
    "        self.ridge={\"alpha\": [0.01, 0.1, 1.0, 10.0, 50.0, 100.0]}\n",
    "        self.elasticNet={\"alpha\":[0.00001, 0.0001, 0.001,0.01, 0.1, 1.0], \n",
    "                         \"l1_ratio\":[0.1, 0.5, 0.7, 0.8, 0.9, 1.0]}\n",
    "        self.parameters = {\"lasso\": self.lasso, \"elasticNet\": self.elasticNet, \"ridge\": self.ridge}\n",
    "        ### models for grid search\n",
    "        self.models = {\"lasso\": lm.Lasso(max_iter=100000), \"elasticNet\": lm.ElasticNet(max_iter=100000), \n",
    "                  \"ridge\": lm.Ridge(max_iter=100000)}\n",
    "        # result containing best models and R-square\n",
    "        self.res=[]\n",
    "        \n",
    "    def error(self, model, X, y):\n",
    "        e = np.sum((model.predict(X) - y)**2) / len(y)\n",
    "        return e\n",
    "\n",
    "    def scorer(self, model, X, y):\n",
    "        '''custom scorer'''\n",
    "        \n",
    "        s = 1 - np.sum( (model.predict(X) - y)**2) / np.sum((y.mean() - y)**2 )\n",
    "        return s\n",
    "\n",
    "    def preprocess(self):\n",
    "        '''preprocess matrix features removing output, features in l and filling nan'''\n",
    "        \n",
    "        X = self.matrix_df.drop([\"prevalence_c\"] , axis=1, inplace=False)\n",
    "        try:\n",
    "            X = self.matrix_df.drop([\"prevalence_a\"] , axis=1, inplace=False)\n",
    "        except:\n",
    "            None\n",
    "            \n",
    "        X = self.matrix_df.drop(self.l , axis=1, inplace=False)\n",
    "\n",
    "        self.y_c = self.matrix_df[\"prevalence_c\"]\n",
    "        try:\n",
    "            self.y_a = self.matrix_df[\"prevalence_a\"]\n",
    "        except:\n",
    "            self.y_a = None\n",
    "\n",
    "        X = X.fillna(X.mean())\n",
    "        return X\n",
    "\n",
    "    def split_scale(self, X, y):\n",
    "        '''split in train/test and scale features'''\n",
    "\n",
    "        scaler=RobustScaler()\n",
    "        if self.cv_type == \"k\":\n",
    "            self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=self.test_size, random_state=self.r)\n",
    "            self.X_train = scaler.fit_transform(self.X_train)\n",
    "            self.X_test = scaler.transform(self.X_test)\n",
    "        else:\n",
    "            self.X_train=shuffle(X)\n",
    "            self.y_train=shuffle(y)\n",
    "            self.X_test=[]\n",
    "            self.y_test=[]\n",
    "\n",
    "        self.X_train=scaler.fit_transform(self.X_train)\n",
    "\n",
    "    def k_fold_test(self, model):\n",
    "        '''perform k-fold cv on test set'''\n",
    "        \n",
    "        n = len(self.y_test) // self.k    \n",
    "        R=[]\n",
    "        #tot=0\n",
    "        for i in range(0,self.k-1):\n",
    "\n",
    "            X_sample=self.X_test[(i*n):((i+1)*n)]\n",
    "            y_sample=self.y_test[(i*n):((i+1)*n)]\n",
    "            r=self.scorer(model, X_sample, y_sample)\n",
    "            R.append(r)\n",
    "\n",
    "        X_sample=self.X_test[((self.k-1)*n):]\n",
    "        y_sample=self.y_test[((self.k-1)*n):]\n",
    "        r=self.scorer(model, X_sample, y_sample)\n",
    "        R.append(r)\n",
    "        return np.mean(R)\n",
    "\n",
    "    def routine_grid(self, model, par):\n",
    "        '''routine for cross-validation with different hyper-parameters'''\n",
    "        \n",
    "        grid = GridSearchCV(model, par, scoring=\"r2\", cv=5)\n",
    "        grid.fit(self.X_train, self.y_train)\n",
    "        \n",
    "        # we select the model\n",
    "        model = grid.best_estimator_\n",
    "        if self.cv_type==\"k\":\n",
    "            model.fit(self.X_train, self.y_train)  \n",
    "            best_score = model.score(self.X_train, self.y_train)\n",
    "            # we compute the R-square on the test set\n",
    "            test_score = self.k_fold_test(model)\n",
    "        else:\n",
    "            best_score = model.score(self.X_train, self.y_train)\n",
    "            test_score = None\n",
    "\n",
    "        return model, best_score, test_score\n",
    "\n",
    "    def main_grid(self):\n",
    "        '''regression model selection'''\n",
    "        \n",
    "        X = self.preprocess()\n",
    "        if self.flag==\"c\":\n",
    "            self.split_scale(X, self.y_c)\n",
    "        else:\n",
    "            self.split_scale(X, self.y_a)\n",
    "\n",
    "        for m in [\"lasso\", \"elasticNet\", \"ridge\"]:\n",
    "\n",
    "            model = self.models[m]\n",
    "            par = self.parameters[m]\n",
    "            \n",
    "            model, best_score, test_score = self.routine_grid(model, par)\n",
    "            self.res.append([model,best_score,test_score])\n",
    "            print(m)\n",
    "            \n",
    "    def getResult(self):\n",
    "        return self.res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matrix_df=pd.read_csv(\"../../data/matrix/matrix_df_163.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(306, 163)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model=Model(matrix_df, r=np.random.randint(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lasso\n",
      "elasticNet\n",
      "ridge\n"
     ]
    }
   ],
   "source": [
    "model.main_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Lasso(alpha=0.001, copy_X=True, fit_intercept=True, max_iter=100000,\n",
       "     normalize=False, positive=False, precompute=False, random_state=None,\n",
       "     selection='cyclic', tol=0.0001, warm_start=False),\n",
       "  0.99999710958202792,\n",
       "  0.99999520889739324],\n",
       " [ElasticNet(alpha=0.001, copy_X=True, fit_intercept=True, l1_ratio=1.0,\n",
       "        max_iter=100000, normalize=False, positive=False, precompute=False,\n",
       "        random_state=None, selection='cyclic', tol=0.0001, warm_start=False),\n",
       "  0.99999710958202792,\n",
       "  0.99999520889739324],\n",
       " [Ridge(alpha=0.01, copy_X=True, fit_intercept=True, max_iter=100000,\n",
       "     normalize=False, random_state=None, solver='auto', tol=0.001),\n",
       "  0.99999195115637385,\n",
       "  0.9998851887407203]]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.getResult()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### repeat for asthma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matrix_asthma=pd.read_csv(\"./data/matrices_features/matrix_asthma.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(663, 35)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_asthma.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model=Model(matrix_asthma, r=np.random.randint(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'prevalence_c'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/home/georgos/anaconda3/lib/python3.5/site-packages/pandas/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2133\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2134\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2135\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:4433)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:4279)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/src/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:13742)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/src/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:13696)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'prevalence_c'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-eea6590494b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-52-74e2bd19ee29>\u001b[0m in \u001b[0;36mmain_grid\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;34m'''regression model selection'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_scale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-52-74e2bd19ee29>\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"prevalence_c\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"prevalence_c\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"prevalence_a\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/georgos/anaconda3/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2057\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2058\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2059\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2061\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/georgos/anaconda3/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2064\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2065\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2066\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2067\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2068\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/georgos/anaconda3/lib/python3.5/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1384\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1386\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1387\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/georgos/anaconda3/lib/python3.5/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   3541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3542\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3543\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3544\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3545\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/georgos/anaconda3/lib/python3.5/site-packages/pandas/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2134\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2135\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2136\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2138\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:4433)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:4279)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/src/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:13742)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/src/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:13696)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'prevalence_c'"
     ]
    }
   ],
   "source": [
    "model.main_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Lasso(alpha=0.0001, copy_X=True, fit_intercept=True, max_iter=100000,\n",
       "     normalize=False, positive=False, precompute=False, random_state=None,\n",
       "     selection='cyclic', tol=0.0001, warm_start=False),\n",
       "  0.99999998738474316,\n",
       "  0.99999998571322823],\n",
       " [ElasticNet(alpha=0.0001, copy_X=True, fit_intercept=True, l1_ratio=1.0,\n",
       "        max_iter=100000, normalize=False, positive=False, precompute=False,\n",
       "        random_state=None, selection='cyclic', tol=0.0001, warm_start=False),\n",
       "  0.99999998738474316,\n",
       "  0.99999998571322823],\n",
       " [Ridge(alpha=0.01, copy_X=True, fit_intercept=True, max_iter=100000,\n",
       "     normalize=False, random_state=None, solver='auto', tol=0.001),\n",
       "  0.99999999762589153,\n",
       "  0.9999999969624952]]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.getResult()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# r_lasso=[]\n",
    "# r_ridge=[]\n",
    "# r_elastic=[]\n",
    "\n",
    "# for i in range(10):\n",
    "#     res=main_grid(matrix_df)\n",
    "#     r_lasso.append(res[0][2])\n",
    "#     r_elastic.append(res[1][2])\n",
    "#     r_ridge.append(res[2][2])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
